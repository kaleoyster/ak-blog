---
layout: post
title: LSTM autoencoder for reconstruction and prediction of Moving MNIST data
description: Long-short-term memory architecture have in learning representation of sequence data.
---

Classical machine learning algorithms and neural networks, in particular, are designed to work with fixed length inputs. Therefore, learning the observation’s temporal ordering can make it challenging to extract features suitable for use as input to supervised learning models. 

[Link to the Github Project](http://www.unexpected-vortices.com/sw/rippledoc/quick-markdown-example.html){:target="_blank", style="align:center"}

## Idea 
- Classical machine learning algorithms and neural networks, in particular, are designed to work with fixed length inputs. Therefore, learning the observation’s temporal ordering can make it challenging to extract features suitable for use as input to supervised learning models. 
- Prediction modeling problems involving sequences data requires to produce a sequence as a prediction. Researchers have developed specific networks that support the sequence data and learn temporal features.
- Recurrent neural networks (RNN) consist of feedback loops in their recurrent layer, enabling Recurrent neural networks to learn temporal patterns. 
- However, RNN is difficult to train because of the loss of gradient with time. Long-short term memory (LSTM) networks are an extension of RNNs that solve the vanishing gradient problem. 
- The architecture of LSTMs can be organized to learn complex prediction problems that involve sequence data of various types and lengths. 
- The autoencoder LSTMs particularly support the variable’s length. 
- In this project, LSTM autoencoder will be the basis for learning a complex step by step temporal representation of sequence data.

![Learning of the sequence model ](https://images.unsplash.com/photo-1488190211105-8b0e65b80b4e?w=500&h=500&fit=crop "An exemplary image")
=

## Conclusion
- LSTM autoencoders can learn a compressed representation of video sequence data. 
- This project demonstrates the learning of sequence data on moving MNIST dataset. 
- From the results, the overall performance of Reconstruction of LSTM autoencoder was better than prediction LSTM autoencoder concerning the reconstruction of each frame of sequence data Reconstruction. 
- Also, notice that the prediction LSTM autoencoder model struggled to properly reconstruct digits and foreground properly, resulting from overfitting of the model, or this may also be due to lack of learning of features to reconstruct foreground. 
-  To address this issue of reconstruction, future work could address this issue on employing pre-trained models of MNIST dataset to learn temporal patterns or reformat frames into black and white color instead of three channels as it may be easier for the model to learn temporal features
